#### 分库分表

##### 1、为什么要进行分库分表？

- 考察点1：为什么要分库分表（设计高并发系统的时候，数据库层面如何设计？）

- 考察点2：用过哪些分库分表中间件？他们的优势和劣势是什么？

- 考察点3：具体如何对数据库进行垂直拆分或水平拆分的？

​		分库分表一定是为了支撑高并发、数据量大两个问题。具体来说，可能分表不分库，也可能分库不分表。

- **分表**：单表数据量过大，会极大影响你的sql执行性能（一般单表几百万数据）

- **分库**：一个库最多支撑并发2000,，就需要扩容了，一个健康的单库并发最好保持在每秒1000左右

​        **常见分库分表中间件：Sharding-jdbc、Mycat**

​		sharding-jdbc：当当开源，Client层方案，支持分库分表、读写分离、分布式id生成、柔性事务（最大努力送达型事务、TCC事务）。优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能相对较高；缺点是升级时需要各个系统都重新升级版本再发布，各个系统都耦合了sharding-jdbc的依赖。

​		Mycat：基于Cobar改造，proxy层方案，功能完善。优点是各个项目之间是透明的，不需要管Mycat的升级版本等问题；缺点是需要部署，自己运维一套中间件，成本高。

​		水平拆分：将单表的数据分担到多个库的多个表里，表的结构都一样，所有表的数据加起来才是全部数据。（作用就是用多个库来扛更高的并发，以及用多个库的存储容量来进行扩容）

​		![database-split-horizon](https://doocs.github.io/advanced-java/docs/high-concurrency/images/database-split-horizon.png)

​		垂直拆分：将单表的很多字段进行拆分成多个表，或者是多个库，每个表的结构不同（几张不同的关联表）；一般会将较少访问频率高的字段放入一个表，将较多的访问频率较少的字段放入另外一个表

![database-split-vertically](https://doocs.github.io/advanced-java/docs/high-concurrency/images/database-split-vertically.png)

​		上述中间件都支持分库分表，中间件可以根据你指定的某个字段值，比如说userid，自动路由到对应的库，然后再自动路由到对应的表里去。

​		分库分表的方式：

1. range：每个库一段连续的数据，比如时间范围。优点是：扩容简单，如每个月一个库；缺点是这种方法容易产生热点问题，即流量都打在最新数据上。
2. hash：按照某字段hash一下均匀分散，较为常用；优点是每个库的数据量和请求压力平均分布，缺点是扩容麻烦，在数据迁移时，之前的数据需要重新计算hash值并重新分配到不同的库或表

##### 2、分库分表如何平滑过渡？

- 考察点：即如何设计才能让系统从单库单表**动态切换**到分库分表上？

######          分库分表的常见步骤

- 选择一个数据库中间件，调研、学习、测试；
- 设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，比如 3 个库，每个库 4 个表；
- 基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写；
- 完成单库单表到分库分表的**迁移**，双写方案；
- 线上系统开始基于分库分表对外提供服务；

######         停机迁移方案

​		挂公告：0点~6点运维，无法访问；0点时，系统停掉，提前写好**导数的一次性工具**，此时跑起来，将单库单表的数据读出来并写入分库分表里；导数完毕后，修改数据库连接配置、代码、SQL等。（比较low的方案）

![database-shard-method-1](https://doocs.github.io/advanced-java/docs/high-concurrency/images/database-shard-method-1.png)

###### 		双写迁移方案

​		在线上系统，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新库的增删改，这就是所谓的**双写**，即同时写老库和新库。

​		系统部署后，新库数据肯定差很多，用上文说的导数工具，跑起来读老库数据写新库，写的时候根据gmt_modified这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写（即不允许老数据覆盖新数据）

​		导完一轮后，有可能数据还是不一致，需要程序自动做一轮校验，若有不一样的，从老库读数据再次写，反复循环，直到两个库所有表的数据一致为止。

![database-shard-method-2](https://doocs.github.io/advanced-java/docs/high-concurrency/images/database-shard-method-2.png)

##### 3、如何设计可以动态的扩容方案？

- 考察点：在分库分表完成后，已经建立好的库和表支撑不住了，如何进行扩容？

######         停机扩容（不推荐）

​		步骤类似停机迁移，唯一不同是导数工具需要把现有库表的数据抽取慢慢倒入新的库表，但不推荐，一般来说，分库分表就是因为数据量过大；在单库单表到分库分表的时候，数据量可能不是很大，单表可能就两三千万，可以写个工具，多弄几台机器并行跑，1小时可能就完事了；倘若3个库+12张表，数据量达到亿级别，导数据就得几个小时，后续还得修改配置，重启系统，测试验证...

###### 		优化方案

​		可以一开始就32库+32表，即1024张表（无论并发支撑或数据量支撑都没问题），利用模32的方法进行路由

​		假设每个库正常承载写入并发量1000,32个库就是3.2w，如果每个库承载1.5k的写并发，即可以达到1.5*32=4.8w的写并发，再加一个MQ，削峰，每秒写入MQ8w条，每秒消费5w条；

​		对于数据量，每个表放500万，那么可以放50亿条数据

​		开始可能这个库是逻辑库，即一个MySQL服务器建了n个库，比如32，后面拆分就是在库和MySQL服务器之间做迁移，系统改一下配置；这样最多可以扩展到32个MySQL服务器，每个数据库服务器是一个库；若还不够，最多可以扩展到1024个服务器，每个数据库一张表；若需要减少库的数量，只要按倍数缩容就行了，修改一下路由规则

​		步骤：

1. 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32 库 * 32 表，对于大部分公司来说，可能几年都够了
2. 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表
3. 扩容的时候，申请增加更多的数据库服务器，装好 MySQL，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器
4. 由 DBA 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的
5. 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址
6. 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务

##### 4、分库分表之后，id主键如何处理？（分布式id）

- 考察点：分成多个表之后，每个表的id都是从1开始累加，这时需要一个全局唯一的id来支持

######         数据库自增id

​		系统每次得到一个id，往一个库的一个表里插入一条无含义的业务数据，然后获取数据库自增的这个id，拿到这个id后再到分库分表里进行写入。

​		好处是方便简单，缺点是**单库生成**自增id，高并发场景下不适用；改进：开一个服务，这个服务拿到当前最大id值，自己递增几个id，一次性返回一批id，然后再把最大id修改成递增几个id最大的一个值，但都是基于单库

​		适合场景：若是由于**并发不高、数据量过大**导致的分库分表扩容，可以采用这个方案

###### 		设置数据库sequence或者表自增字段步长

​		可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。

​		比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。

![database-id-sequence-step](https://doocs.github.io/advanced-java/docs/high-concurrency/images/database-id-sequence-step.png)

​		适合场景：在用户防止产生id重复时，可以使用；但服务节点固定、步长固定，若需要增加节点，就不好操作了

###### 		UUID

​		`UUID.randomUUID().toString().replace("-", "") -> sfsdf23423rr234sfdaf`

​		上述两种方式都是基于数据库，这种就是本地生成；缺点是UUID过长，占用空间大，且**作为主键性能太差**；更重要的是，UUID不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。

​		适合场景：随机生成文件名、编号等，主键不能用UUID

###### 		获取系统当前时间

​		在高并发条件下，比如一秒并发几千，会有重复情况，不用考虑

​		适合场景：可以将当前时间+业务字段组合作为id

###### 		snowflake算法

​		twitter开源的分布式id生成算法，采用Scala语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bits 作为毫秒数，用 10 bits 作为工作机器 id，12 bits 作为序列号

​		**工具类可参考**https://doocs.github.io/advanced-java/#/./docs/high-concurrency/database-shard-global-id-generate

```
0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000
```

- 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。
- 41 bits：表示的是时间戳，单位是毫秒。41 bits 可以表示的数字多达 `2^41 - 1` ，也就是可以标识 `2^41 - 1` 个毫秒值，换算成年就是表示69年的时间。
- 10 bits：记录工作机器 id，代表的是这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。但是 10 bits 里 5 个 bits 代表机房 id，5 个 bits 代表机器 id。意思就是最多代表 `2^5` 个机房（32 个机房），每个机房里可以代表 `2^5` 个机器（32台机器）。
- 12 bits：这个是用来记录同一个毫秒内产生的不同 id，12 bits 可以代表的最大正整数是 `2^12 - 1 = 4096` ，也就是说可以用这个 12 bits 代表的数字来区分**同一个毫秒内**的 4096 个不同的 id。