# 前言

## 为什么要学习数据结构和算法

**目的：**

- 大厂面试必备
- 业务开发可能不需要，但并不意味着什么都不需要了解（基础框架的设计思想，阅读源码）
- 写出开源框架的代码才是程序员的追求
- 能够提升代码的性能，能够很好的训练自己的思维，多辩证思考，多问为什么

（PS：很多大龄候选人，简历能写十几页，经历的项目有几十个，但是细看下来，每个项目都是重复地堆砌业务逻辑而已，完全没有难度递进，看不出有能力提升。久而久之，十年的积累可能跟一年的积累没有任何区别。这样的人，怎么不会被行业淘汰呢？）

**如何学习数据结构与算法**

1. 为什么会有这种数据结构或算法，它有什么特点？
2. 适合用来解决哪些问题？
3. 有哪些实际的应用场景？

**学习技巧**

1. 边学边练，适度刷题
2. 多问，多思考，多互动
3. 知识需要沉淀，不可能一下子融会贯通

**学习工具**

leetCode、算法可视化（https://www.cs.usfca.edu/~galles/visualization/Algorithms.html）

**数据结构与算法全景图**

![](../../picture/basic_data_structures_and_algorithms.jpg)

# 复杂度分析

数据结构与算法解决的是如何让代码运行的更快，如何更加节省存储空间

**如何分析、统计算法的执行效率和资源消耗**

**为什么需要复杂度分析？**

通过事后对算法的执行时间和占用内存进行统计、监控来得到算法执行的时间和内存大小，叫**事后统计法**，这种评估算法执行效率的方法是对的，但存在一些局限性

- 测试结果非常依赖测试环境（处理器的不同）
- 测试结果受数据规模的影响很大（例如小规模数据可能插入排序比快速排序还快，又或待排序数据的有序度也会影响执行时间）

所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法，即大O复杂度表示法

**大O复杂度表示法**

表示代码执行时间随数据规模增长的变化趋势，所以，也叫作**渐进时间复杂度**（asymptotic time complexity），简称**时间复杂度**。如T(n) = O(n)； T(n) = O(n2)。

**时间复杂度分析**：

1. 只关注循环执行次数最多的一段代码
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

![](F:\github\super-java\picture\设计模式之美-时间复杂度.jpg)

对于刚罗列的复杂度量级，我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2n) 和 O(n!)。**当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。**

**空间复杂度分析**：**渐进空间复杂度**（asymptotic space complexity），**表示算法的存储空间与数据规模之间的增长关系**，常见的空间复杂度就是 O(1)、O(n)、O(n2 )

**浅析最好、最坏、平均、均摊时间复杂度**

```
// n表示数组array的长度
int find(int[] array, int n, int x) { 
	int i = 0; 
	int pos = -1; 
	for (; i < n; ++i) { 
		if (array[i] == x) { 
			pos = i; 
			break;   //此处结束可以结束循环，节省时间
		} 
	} 
	return pos;
}
```

- 最好、最坏情况时间复杂度

最好情况就是在数组第一个元素就匹配到了，最坏即在数组最后一个元素才找到x

- 平均情况时间复杂度

要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0～n-1 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即：

![平均时间复杂度](F:\github\super-java\picture\平均时间复杂度.jpg)

其实在0~n-1位置中和不在数组中的概率这样统计并不好，但可以认为其时间复杂度为O(n)

- 均摊时间复杂度

```
// array表示一个长度为n的数组 
// 代码中的array.length就等于n 
	int[] array = new int[n]; 
	int count = 0; 
	void insert(int val) { 
		if (count == array.length) { 
			int sum = 0; 
			for (int i = 0; i < array.length; ++i) { 
				sum = sum + array[i]; 
			} 
			array[0] = sum; 
			count = 1; 
		} 
		array[count] = val; 
		++count; 
	}
```

最好是直接插入，即O(1)，最坏情况是数组已满，需要求和清空数组，即O(n)

平均时间复杂度是1\*1/n+1+....+n\*1/n+1，即O(1)

每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。

**摊还分析法**：对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这时看**是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上**。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。

# 基础数据结构

## 数组

**为什么数组要从 0 开始编号，而不是从 1 开始呢？**

```
a[k]_address = base_address + k * type_size
```

”下标”最确切的定义应该是“偏移（offset）”，即可用来计算内存地址；假若用1编号，则a[k]_address = base_address + （k-1）* type_size，**多了一次减法运算，CPU需要多做一次操作**。

**如何实现随机访问？**

> 数组（Array）是一种**线性表**数据结构。它用一组**连续**的内存空间，来存储一组具有**相同类型**的数据。

特点：

- **线性表**：每个线性表上的数据最多只有前和后两个方向，例如数组、链表、队列、栈等，与之相对应的则是非线性表（二叉树、堆、图等）。
- **连续的内存空间和相同类型的数据**：因此数组支持**随机访问**，**根据下标**随机访问的时间复杂度为 O(1)，并不是查询的时间复杂度为O(1)。误区：查找时间复杂度并不是O(1)，即使是已排序的数组，用二分查找，时间复杂度也是O(logn)。

**低效的插入和删除**

最好情况在末尾，故最好时间复杂度为 O(1)；若数据在开头，则所有数据则都需后移一位，为O(n)；平均时间复杂度为（1+2+3+...+n）/n，即O(n)。

优化：

1. 在数组只是被当成一个存储数据的集合时，进行**插入**时，可以将某一位置的元素移至末尾，再添加元素到指定位置，这样复杂度就变为了O(1)；
2. 在不追求数组的连续性时，**删除**元素时，可以将多个位置的元素先**标记**，最终进行一次删除操作。--->>>JVM 标记清除垃圾回收算法的核心思想

**警惕数组的访问越界问题**

```c
int main(int argc, char* argv[]){
    int i = 0;
    int arr[3] = {0};
    for(; i<=3; i++){
        arr[i] = 0;//即当i=3时，arr[3]即为变量i的地址，即i=0，导致无限循环，同类型才会出现
        printf("hello world\n");
    }
    return 0;
}
```

分析：函数体内的局部变量存在栈上，且是连续压栈。在Linux进程的内存布局中，栈区在高地址空间，从高向低增长，即栈底地址大。变量i和arr在相邻地址，且i比arr的地址大，所以arr越界正好访问到i。当然，前提是i和arr元素同类型，否则这段代码仍是未决行为。

**容器能否完全替代数组？**

在Java中，ArrayList 最大的优势就是**可以将很多对数组操作的细节封装起来**，并且还**支持动态扩容**

代码规范：扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，**如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小**。

数组的使用场景：

1. Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组
2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组

## 链表

**如何实现LRU缓存淘汰算法？**

缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）

解决方法：

我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。

1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。

2. 如果此数据没有在缓存链表中，又可以分为两种情况：

   - 如果此时缓存未满，则将此结点直接插入到链表的头部；
   - 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。

   **优化方案**：引入散列表，记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)

**底层存储结构**

链表是由一个个独立的内存块组成的，链表可分为单链表、循环链表、双向链表

1. 单链表：插入和删除的时间复杂度是O(1)，随机访问的时间复杂度为O(n)
2. 循环链表：尾结点指向头结点。优点是当要处理的数据具有环型结构特点时，就特别适合采用循环链表，比如著名的**约瑟夫问题**
3. 双向链表：不仅有后继指针next，还有前驱指针prev。双向链表可以支持 O(1) 时间复杂度的情况下找到**前驱结点**，对插入跟删除比较有利；对于**有序链表**的查找效率也较高一些。Java中的LinkedHashMap就使用了双向链表，利用了**空间换时间**的设计思想

**链表VS数组**

数组的优势在于使用的是连续的存储空间，可以借助CPU的缓存机制，随机访问效率高；缺点是大小固定，若数组过大，可能没有连续的内存空间分配，若数组过小，则可能出现空间不够用，此时进行扩容，原数组的拷贝非常耗时。而链表本身没有大小限制，天然支持动态扩容。

若代码对内存的使用比较苛刻，则建议使用数组，因为链表每个节点需要使用额外的空间存储指针，而且链表频繁的插入、删除会导致频繁的内存申请和释放，容易造成内存碎片，就可能导致频繁的GC。

**写链表代码技巧**

- 理解指针或引用的含义

- 警惕指针丢失和内存泄漏

- 利用哨兵简化实现难度

  针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理

  引入哨兵结点，在任何时候，不管链表是不是空，head 指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫**带头链表**。相反，没有哨兵结点的链表就叫作**不带头链表**

- 重点留意边界条件处理

- 举例画图，辅助思考

- 多写多练，没有捷径

  - 单链表反转
  - 链表中环的检测
  - 两个有序的链表合并
  - 删除链表倒数第 n 个结点
  - 求链表的中间结点

## 栈

**如何实现浏览器的前进和后退功能？**

利用两个栈实现，一个栈存储浏览过的页面，另一个存储返回后弹出的数据

**如何理解"栈"?**

后进者先出，先进者后出，就是典型的"栈"结构。

从栈的操作特性上来看，**栈是一种“操作受限”的线性表**，只允许在一端插入和删除数据

当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构

**如何实现一个“栈”？**

栈主要包括入栈、出栈两个操作，用数组实现的栈叫**顺序栈**，用链表实现的栈叫**链式栈**

入栈和出栈都只涉及栈顶数据的操作，故栈的时间复杂度和空间复杂度都是O(1)

**支持动态扩容的顺序栈**

基于数组的栈是固定大小的，而基于链表的栈需存储指针，消耗内存

支持动态扩容后，出栈操作时间复杂度仍为O(1)，而入栈涉及扩容，将耗时多的入栈操作的时间均摊到其他入栈操作上，时间复杂度也接近O(1)

**栈的应用场景**

- 函数调用栈
- 表达式求值
- 括号匹配

**JVM中的"堆栈"和数据结构中的"栈"是一回事吗？**

## 队列

**队列在线程池等有限资源池中的应用**

CPU 资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反而会导致 CPU 频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点和硬件环境，来事先设置的

当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？

一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理

基于链表实现的无界队列可能会导致过多的请求排队等待，不适合对响应时间比较敏感的系统；基于数组实现的有界队列，队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能

实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队

**如何理解"队列"？**

**先进者先出**，队列也只支持两个操作，入队enqueue()：放一个数据到队列尾部；出队dequeue()：从队列头部取一个元素

队列也是一种**操作受限的线性表数据结构**

**顺序队列和链式队列**

用数组实现的队列叫作**顺序队列**，用链表实现的队列叫作**链式队列**

**循环队列**

队列为空的判断条件是 head == tail，队满时，**(tail+1)%n=head**

**阻塞队列和并发队列**

**阻塞队列**其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回

线程安全的队列我们叫作**并发队列**。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因

## 递归

**递归需要满足的三个条件**

1. 一个问题的解可以分解为几个子问题的解
2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
3. 存在递归终止条件

编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤

**递归代码要警惕堆栈溢出**

如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险；可以通过在代码中限制递归调用的最大深度的方式来解决这个问题，但这种方法并不是很实用

**递归代码要警惕重复计算**

想要计算 f(5)，需要先计算 f(4) 和 f(3)，而计算 f(4) 还需要计算 f(3)，因此，f(3) 就被计算了很多次，这就是重复计算问题

**怎么将递归代码改写为非递归代码？**

改为这种**迭代循环**的非递归写法

## 树

**二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？**

**树**

```
节点的高度：节点到叶子节点的最长路径（边数）
节点的深度：根节点到这个节点所经历的边的个数
节点的层数：节点的深度+1
树的高度：根节点的高度
```

**二叉树**

二叉树，顾名思义，每个节点最多有两个“叉”，也就是两个子节点，分别是**左子节点**和**右子节点**

```
满二叉树：叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点
完全二叉树：叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大
```

二叉树的存储也是分为链式存储和顺序存储；因为该二叉树是一颗完全二叉树，则用数组存储会节省内存，因为链表需要额外的开销，而数组连续存储，且完全二叉树在数组存储中不怎么浪费空间。

**二叉树的遍历**

```
前序遍历：对于树中的任意节点，先打印这个节点，然后再打印它的左子树，最后打印它的右子树
中序遍历：对于树中的任意节点，先打印它的左子树，然后再打印它本身，最后打印它的右子树
后序遍历：对于树中的任意节点，先打印它的左子树，然后再打印它的右子树，最后打印节点本身
```

**实际上，二叉树的前、中、后序遍历就是一个递归的过程**，二叉树遍历的时间复杂度是 O(n)

**二叉查找树**

二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作

二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。

**既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？**

散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)

1. 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列
2. 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)
3. 笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高
4. 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定
5. 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间

**支持重复数据的二叉查找树**

1. 第一种方法比较容易。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。
2. 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树

**二叉查找树的时间复杂度分析**

当左右子树极度不平衡时，就会退化成链表，所以查找时间复杂度就是O(n)

当二叉查找树是一颗完全二叉树时，**时间复杂度其实都跟树的高度成正比，也就是 O(height)**，即完全二叉树的高度小于等于 log2n

极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树，即二叉平衡查找树，平衡二叉查找树的高度接近 logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是 O(logn)

**平衡二叉查找树**

**为什么工程中都喜欢用红黑树，而不是其他平衡二叉查找树呢？**

 Treap、Splay Tree，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化

AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了

**什么是“平衡二叉查找树”？**

二叉树中任意一个节点的**左右子树的高度**相差不能大于 1

发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题

**平衡**：只要看起来对称，左右子树高度不要相差太多，这样整棵树的高度就相对低，相应的插入、删除、查找等操作的效率高一些

**如何定义一棵“红黑树”？**

平衡二叉查找树有Splay Tree（伸展树）、Treap（树堆）等，但红黑树更广为人知，其实，红黑树是一种不严格的平衡二叉查找树

1. 根节点是黑色的；
2. 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
3. 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
4. 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；

**为什么说红黑树是“近似平衡”的？**

“平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化的太严重。

红黑树的高度近似 2log2n，实际上红黑树的性能更好

**实现红黑树的基本思想**

主要是通过左旋、右旋将上述3、4点恢复

**插入操作的平衡调整**

**删除操作的平衡调整**

插入操作的平衡调整比较简单，但是删除操作就比较复杂

**递归树**

**递归树与时间复杂度分析**

归并排序递归实现的时间复杂度就是O(nlog⁡n)

**实战一：分析快速排序的时间复杂度**

从概率论的角度来说，快排的平均时间复杂度就是 O(nlogn)

**实战二：分析斐波那契数列的时间复杂度**

这个算法的时间复杂度就介于 O(2^n)和 O(2^(n/2) 之间

**实战三：分析全排列的时间复杂度**

全排列的递归算法的时间复杂度大于 O(n!)，小于 O(n∗n!)

**堆**

堆是一种**完全二叉树**，堆排序是一种原地的、时间复杂度为 O(nlog⁡n) 的排序算法。快速排序，平均情况下，它的时间复杂度为 O(nlogn)。尽管这两种排序算法的时间复杂度都是 O(nlog⁡n)，甚至堆排序比快速排序的时间复杂度还要稳定，但是，**在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？**

1. 堆排序数据访问的方式没有快速排序友好。
2. 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序

**如何理解“堆”？**

- 堆是一个完全二叉树；
- 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。

**如何实现一个堆？**

先得弄清**堆都支持哪些操作**以及**如何存储一个堆**。

完全二叉树比较适合用数组来存储

- 往堆中插入一个元素---堆化
- 删除堆顶元素

**如何基于堆实现排序？**

堆排序时间复杂度非常稳定，是 O(nlogn)，并且它还是原地排序算法。

堆排序的过程：

1. 建堆
2. 排序

**堆的应用场景**

1. 优先级队列：合并有序小文件、高性能定时器
2. 利用堆求TopK
3. 利用堆求中位数

## 哈希

**你会如何存储用户密码这么重要的数据吗？仅仅 MD5 加密一下存储就够了吗？**

可以通过哈希算法，对用户密码进行加密之后再存储，不过最好选择相对安全的加密算法，比如 SHA 等（因为 MD5 已经号称被破解了）。

字典攻击你听说过吗？如果用户信息被“脱库”，黑客虽然拿到是加密之后的密文，但可以通过“猜”的方式来破解密码，这是因为，有些用户的密码太简单

针对字典攻击，我们可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度

**什么是哈希算法？**

将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是**哈希算法**，而通过原始数据映射之后得到的二进制值串就是**哈希值**

- 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）
- 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同
- 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小
- 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值

**哈希算法应用场景**

哈希算法应用最常见的场景有安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储

1. 安全加密

   常用于加密的哈希算法有**MD5**（MD5 Message-Digest Algorithm，MD5 消息摘要算法）、**SHA**（Secure Hash Algorithm，安全散列算法）、**DES**（Data Encryption Standard，数据加密标准）、**AES**（Advanced Encryption Standard，高级加密标准）

   对于用于加密的哈希算法来说，前面两点更加重要，即很难根据哈希值反向推导出原始数据、散列冲突的概率要很小

   任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长

   选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法

2. 唯一标识

   哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据

3. 数据校验

   用于校验数据的完整性和正确性

4. 散列函数

   散列函数中用到的散列算法，更加关注散列后的值是否能平均分布

   它对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率

5. 负载均衡

   负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上

   **我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。**

6. 数据分片

   我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度

7. 分布式存储

   采用数据分片的思想来做，当需要扩容时，会面临所有的数据都要重新计算哈希值的情况，相当于缓存中的数据全部消失，容易发生缓存穿透，产生雪崩效应。

   这里就可以采用**一致性哈希算法**

## 拓扑排序

**如何确定代码源文件的编译依赖关系？**

**算法解析**

我们可以把源文件与源文件之间的依赖关系，抽象成一个有向图。每个源文件对应图中的一个顶点，源文件之间的依赖关系就是顶点之间的边。实际上，拓扑排序本身就是基于有向无环图的一个算法。拓扑排序有两种实现方法，都不难理解。它们分别是**Kahn 算法**和**DFS 深度优先搜索算法**。

1. **Kahn 算法**

   Kahn 算法实际上用的是贪心算法思想,Kahn 算法的时间复杂度就是 O(V+E)（V 表示顶点个数，E 表示边的个数）。

2. **DFS 算法**

   每个顶点被访问两次，每条边都被访问一次，所以时间复杂度也是 O(V+E)。

3. 待补充

# 高级数据结构

## 字典树

**如何实现搜索引擎的搜索关键词提示功能？**

**什么是“Trie 树”？**

Trie 树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。

Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起

**如何实现一棵 Trie 树？**

Trie 树主要有两个操作，一个是将字符串集合构造成 Trie 树，另一个是在 Trie 树中查询一个字符串。

构建 Trie 树的时间复杂度是 O(n)（n 表示所有字符串的长度和）

构建好 Trie 树后，查找字符串的时间复杂度是 O(k)，k 表示要查找的字符串的长度

**Trie 树真的很耗内存吗？**

待补充

**Trie树与散列表、红黑树的比较**

Trie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串，也就是类似开篇问题的那种场景

## 跳表

问题：**Redis 为什么会选择用跳表来实现有序集合呢？** 为什么不用红黑树呢？

按照区间来查找数据这个操作，红黑树的效率没有跳表高

**如何理解“跳表”？**

链表+多级索引的结构，就是跳表

**用跳表查询到底有多快？**

第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，第 k索引结点的个数就是 n/(2k)。假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 n/(2h)=2，从而求得 h=log2n-1。如果包含原始链表这一层，整个跳表的高度就是 log2n。我们在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 O(m*logn)。

假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，**每一级索引都最多只需要遍历 3 个结点**。

**跳表是不是很浪费内存？**

空间复杂度还是 O(n)

**高效的动态插入和删除**

动态的插入、删除操作的时间复杂度也是 O(logn)

**跳表索引动态更新**

跳表是通过随机函数

## 散列表

问题：Word文档中的单词拼写检查功能是如何实现的？

**散列思想**

散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据

**散列函数**

1. 散列函数计算得到的散列值是一个非负整数
2. 如果 key1 = key2，那 hash(key1) == hash(key2)；
3. 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。（几乎不可能）

**散列冲突**

1. 开放寻址法

   开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？

   - 线性探测：每次探测的步长是 1
   - 二次探测：探测的步长就变成了原来的“二次方”
   - 双重散列：使用一组散列函数直到找到空闲位置

   不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用**装载因子**（load factor）来表示空位的多少

   ```
   散列表的装载因子 = 填入表中的元素个数 / 散列表的长度
   ```

   装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。

2. 链表法

   插入的时间复杂度是 O(1)，查找或删除操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

结论：散列表的查询效率并不能笼统地说成是 O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。

**如何设计散列函数？**

- 散列函数不能太复杂，消耗计算时间
- 散列函数生成的值要尽可能随机并且均匀分布
- 在实际应用中，还需要考虑关键字长度、特点、分布、散列表大小等

**装载因子过大了怎么办？**

动态扩容，插入一个数据，最好时间复杂度是 O(1)，最坏情况下时间复杂度是 O(n)，均摊情况下，时间复杂度接近最好情况，就是 O(1)

在动态散列表中，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动**动态缩容**。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了

**如何避免低效的扩容？**

一次性扩容消耗时间过大，可将扩容操作穿插在插入操作中。即当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。

**如何选择冲突解决方法？**

1. 开发寻址法

   优点：数据都存储在数组中，可以有效利用CPU缓存加快查询速度，而且这样实现的散列表序列化比较简单

   缺点：删除数据比较麻烦，需要特殊标记已经删除的数据；由于数据存储在数组，冲突相对链表代价更高，因此装载因子上限不能太大，更浪费内存

   **当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因**

2. 链表法

   优点：对内存的利用率高，即链表结点可以在需要的时候再创建；对大装载因子的容忍度更高；

   缺点：对于比较小的对象的存储，消耗内存；结点分散，对CPU缓存不友好，对执行效率有一定影响；

   **基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表**。

**工业级散列表举例分析**

1. 初始大小：HashMap 默认的初始大小是 16，默认值可以设置；如果知道数据量代销可以通过修改默认初始大小，减少动态扩容的次数，可以提高性能

2. 装载因子和动态扩容：最大装载因子默认是 0.75，当超过时，就会启动扩容，每次扩容为2倍大小

3. 散列冲突解决方法：HashMap 底层采用链表法来解决冲突；JDK8时引入了红黑树，当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表

4. 散列函数

   ```
   int hash(Object key) {
       int h = key.hashCode()；
       return (h ^ (h >>> 16)) & (capitity -1); //capicity 表示散列表的大小
   }
   ```

**什么是一个工业级的散列表？应该具有哪些特性？**

- 支持快速的查询、插入、删除操作
- 内存占用合理，不能浪费过多的内存空间
- 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况

**如何实现这样一个散列表呢？**

- 设计一个合适的散列函数
- 定义装载因子阈值，并且设计动态扩容策略
- 选择合适的散列冲突解决方法

**为什么散列表和链表经常在一起使用？**

- LRU 缓存淘汰算法

  借助散列表，我们可以把 LRU 缓存淘汰算法的时间复杂度降低为 O(1)。实现即是新增了一个特殊的字段 hnext，将结点串在散列表的拉链中

- Redis有序集合

  在有序集合中，每个成员对象有两个重要的属性，**key**（键值）和**score**（分值）。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据

  按照键值构建一个散列表，这样按照 key 来删除、查找一个成员对象的时间复杂度就变成了 O(1)。同时，借助跳表结构，其他操作也非常高效。

- Java LinkedHashMap

  LinkedHashMap 也是通过散列表和链表组合在一起实现的，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据

  **LinkedHashMap 是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突**。

散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据

散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用

# 排序算法

**如何分析一个“排序算法”？**

- 排序算法的执行效率

  - 最好情况、最坏情况、平均情况时间复杂度
  - 时间复杂度的系数、常数 、低阶
  - 比较次数和交换（或移动）次数

- 排序算法的内存消耗

  算法的内存消耗可以通过空间复杂度来衡量，不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，**原地排序**，特指**空间复杂度**是 O(1) 的排序算法

- 排序算法的稳定性

  要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个 key 来排序；经过某种排序算法排序之后，如果两个对象的前后顺序没有改变，那我们就把这种排序算法叫作**稳定的排序算法**；如果前后顺序发生变化，那对应的排序算法就叫作**不稳定的排序算法**

## 冒泡排序

> **有序度**：数组中具有有序关系的元素对的个数，如下
>
> ```
> 有序元素对：a[i] <= a[j], 如果 i < j。
> ```
>
> **满有序度**：对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是**n\*(n-1)/2**，也就是 15，即满有序度
>
> 逆序度正好与有序度相反，**逆序度 = 满有序度 - 有序度**

冒泡排序是原地排序算法，也是稳定的，最好情况时间复杂度是 O(n)，最坏情况时间复杂度为 O(n2)

对于包含 n 个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度是 0，所以要进行 n*(n-1)/2 次交换。最好情况下，初始状态的有序度是 n*(n-1)/2，就不需要进行交换。我们可以取个中间值 n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况

平均情况下，需要 n*(n-1)/4 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是 O(n2)，所以平均情况下的时间复杂度就是 O(n2)

## 插入排序

插入排序是原地排序算法,也是稳定的，最好是时间复杂度为 O(n)，注意，这里是**从尾到头遍历已经有序的数据**，最坏情况时间复杂度为 O(n2)，平均时间复杂度为 O(n2)

**为何插入排序比冒泡排序更受欢迎？**

我们把执行一个赋值语句的时间粗略地计为单位时间（unit_time），然后分别用冒泡排序和插入排序对同一个逆序度是 K 的数组进行排序。用冒泡排序，需要 K 次交换操作，每次需要 3 个赋值语句，所以交换操作总耗时就是 3*K 单位时间。而插入排序中数据移动操作只需要 K 个单位时间

## 选择排序

排序空间复杂度为 O(1)，是一种原地排序算法，不稳定；最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)

## 归并排序

归并排序不是原地排序算法，空间复杂度为O(n)；归并排序是一个稳定的排序算法；归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)

## 快速排序

快速排序是原地排序算法，不稳定，快排的最好时间复杂度是 O(nlogn)，最坏情况下时间复杂度是O(n2)，

T(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)

## 桶排序

时间复杂度是 O(n) ；如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)

桶排序对要排序数据的要求是非常苛刻的。首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。

其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了

**桶排序比较适合用在外部排序中**。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。

## 计数排序

**计数排序其实是桶排序的一种特殊情况**。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。

**不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？**

计数排序只能用在数据范围不大的场景，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数

## 基数排序

基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作

**如何实现一个通用的、高性能的排序函数？**

- 如何选择合适的排序算法？

  线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法

  如果对小规模数据进行排序，可以选择时间复杂度是 O(n2) 的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。

  归并排序并不是原地排序算法，空间复杂度是 O(n);快速排序比较适合来实现排序函数

- 如何优化快速排序？

  如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 O(n2)。实际上，**这种 O(n2) 时间复杂度出现的主要原因还是因为我们分区点选的不够合理**。

  最理想的分区点是：**被分区点分开的两个分区中，数据的数量差不多**。

  较常用、比较简单的分区算法有三数取中法，随机法

- 举例分析排序函数

  qsort() 并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在小规模数据面前，**O(n2) 时间复杂度的算法并不一定比 O(nlogn) 的算法执行时间长**。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法。

# 算法

## 二分查找

问题：假设我们有 1000 万个整数数据，每个数据占 8 个字节，**如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？** 我们希望这个功能不要占用太多的内存空间，最多不要超过 100MB，你会怎么做呢？-----二分查找

二分查找针对的是一个**有序的数据集合**，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0

**二分查找的递归与非递归实现**

**二分查找应用场景的局限性**

- 首先，二分查找依赖的是顺序表结构，简单点说就是数组。

- 其次，二分查找针对的是有序数据。

  二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。针对动态变化的数据集合，二分查找将不再适用

- 再次，数据量太小不适合二分查找。

  如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找

- 最后，数据量太大也不适合二分查找。

  比如，我们有 1GB 大小的数据，如果希望用数组来存储，那就需要 1GB 的连续内存空间

**二分查找的变形问题**

1. 变体一：查找第一个值等于给定值的元素
2. 变体二：查找最后一个值等于给定值的元素
3. 变体三：查找第一个大于等于给定值的元素
4. 查找最后一个小于等于给定值的元素

## 字符串匹配算法

**如何借助哈希算法实现高效字符串匹配？**

**BF 算法**

BF 是 Brute Force 的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法。

时间复杂度很高，是 O(n*m)，但实际生产中非常常用，因为一般主串和模式串都不会太长，其次这个算法思想、实现都很简单，符合KISS原则。大部分情况下就够用

**RK 算法**

全称叫 Rabin-Karp 算法，其实就是BF 算法的升级版，即对每个子串分别求哈希值，然后用子串的哈希值和模式串的哈希值比较，缩减了比较的时间开销。理想条件下，RK算法的时间复杂度是O(n)。

RK算法的时间复杂度依赖于哈希算法的设计，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为 O(n*m)。

**如何实现文本编辑器中的查找功能？**

**BM算法**

它的性能是著名的[KMP 算法](https://zh.wikipedia.org/wiki/克努斯-莫里斯-普拉特算法)的 3 到 4 倍

**核心思想**

在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。

**算法原理分析**

1. **坏字符规则**

   我们从模式串的末尾往前倒着匹配，当我们发现某个字符没法匹配的时候。我们把这个没有匹配的字符叫作**坏字符**（主串中的字符）。BM 算法在最好情况下的时间复杂度非常低，是 O(n/m)

2. **好后缀规则**

**BM 算法代码实现**

待补充

**BM 算法的性能分析及优化**

待补充

**如何借助BM算法轻松理解KMP算法？**

**KMP算法**

KMP 算法的时间复杂度是 O(n+m)

**基本原理**

**失效函数计算方法**

**KMP 算法复杂度分析**

KMP 算法的时间复杂度就是 O(m+n)

## AC自动机

**如何用多模式串匹配实现敏感词过滤功能？**

多模式串匹配算法。

**基于单模式串和 Trie 树实现的敏感词过滤**

BF 算法、RK 算法、BM 算法、KMP 算法都是单模式串匹配算法，只有 Trie 树是多模式串匹配算法

**经典的多模式串匹配算法：AC 自动机**

AC 自动机算法，全称是 Aho-Corasick 算法。AC 自动机实际上就是在 Trie 树之上，加了类似 KMP 的 next 数组，只不过此处的 next 数组是构建在树上罢了。

AC 自动机的构建，包含两个操作：

- 将多个模式串构建成 Trie 树；
- 在 Trie 树上构建失败指针（相当于 KMP 中的失效函数 next 数组）。

待补充

AC 自动机的时间复杂度近似于O(n)

## 贪心算法

**如何用贪心算法实现Huffman压缩编码？**

贪心算法有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim 和 Kruskal 最小生成树算法、还有 Dijkstra 单源最短路径算法。	

**如何理解“贪心算法”？**

待补充

**贪心算法实战分析**

待补充

## 分治算法

**谈一谈大规模计算框架MapReduce中的分治思想**

**如何理解分治算法？**

分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。**分治算法是一种处理问题的思想，递归是一种编程技巧**。

待补充

**分治算法应用举例分析**

待补充

**分治思想在海量数据处理中的应用**	

待补充

## 回溯算法

**如何理解“回溯算法”？**

待补充

**两个回溯算法的经典应用**

待补充

## 动态规划

**动态规划**

动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。

待补充

**动态规划理论：一篇文章带你彻底搞懂最优子结构、无后效性和重复子问题**

待补充

**动态规划实战：如何实现搜索引擎中的拼写纠错功能？**

待补充

